{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "# read access token from environment variable\n",
    "import os\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: 0\n",
      "Device names: []\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "access_token = os.getenv(\"HF_TOKEN\")\n",
    "# if access_token is not None:\n",
    "#     print(f\"Access token: {access_token[:3]}{'*' * 16}\")\n",
    "# else:\n",
    "#     print(\"No access token found.\")\n",
    "    # sys.exit(1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# print all available devices\n",
    "print(f\"Available devices: {torch.cuda.device_count()}\")\n",
    "# print devices names\n",
    "print(\n",
    "    f\"Device names: {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}\"\n",
    ")\n",
    "\n",
    "checkpoints = [\n",
    "    # \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    # \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    # \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "    # \"mistralai/Mistral-7B-v0.3\",\n",
    "    \"EleutherAI/pythia-70m-deduped\",\n",
    "    # \"EleutherAI/pythia-160m-deduped\",\n",
    "    # \"EleutherAI/pythia-410m-deduped\",\n",
    "    # \"EleutherAI/pythia-1b-deduped\",\n",
    "    # \"EleutherAI/pythia-1.4b-deduped\",\n",
    "    # \"EleutherAI/pythia-2.8b-deduped\",\n",
    "    # \"EleutherAI/pythia-6.9b-deduped\",\n",
    "    # \"EleutherAI/pythia-12b-deduped\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model downloads\n",
      "Time elapsed: 569.32 seconds\n",
      "########################################\n",
      "Loading model: EleutherAI/pythia-70m-deduped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model EleutherAI/pythia-70m-deduped loaded successfully\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"max_new_tokens\": 10,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_return_sequences\": 4,\n",
      "  \"output_scores\": true,\n",
      "  \"return_dict_in_generate\": true\n",
      "}\n",
      " {'input_ids': tensor([[39302,   369,  5686]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "Batch size: 1\n",
      "batch beam size 4 curl len 3\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-1.1193, -2.7615, -3.0378, -3.1802]), 'next_beam_tokens': tensor([275, 327, 285,  13]), 'next_beam_indices': tensor([0, 0, 0, 0])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-3.4689, -4.1989, -4.6571, -4.6590]), 'next_beam_tokens': tensor([ 253, 5439,  247, 1457]), 'next_beam_indices': tensor([0, 2, 0, 0])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-4.8214, -5.1045, -6.0554, -6.6224]), 'next_beam_tokens': tensor([ 275, 2816, 1986, 5219]), 'next_beam_indices': tensor([1, 3, 0, 0])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-6.1464, -6.2998, -6.6578, -6.8843]), 'next_beam_tokens': tensor([2077,   13, 3995,  253]), 'next_beam_indices': tensor([2, 1, 3, 0])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-7.5905, -7.7644, -8.1718, -8.1783]), 'next_beam_tokens': tensor([ 13,  15, 285,  13]), 'next_beam_indices': tensor([0, 0, 1, 2])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-8.9743, -9.0202, -9.6897, -9.7068]), 'next_beam_tokens': tensor([187, 285, 533, 285]), 'next_beam_indices': tensor([1, 0, 0, 3])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([ -9.0848, -10.9885, -11.6895, -11.6901]), 'next_beam_tokens': tensor([187, 253, 344, 253]), 'next_beam_indices': tensor([0, 1, 1, 3])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-11.2735, -12.1025, -12.3365, -12.3514]), 'next_beam_tokens': tensor([ 510,    3,  688, 1628]), 'next_beam_indices': tensor([0, 0, 0, 0])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-14.0194, -14.2605, -14.5095, -14.5626]), 'next_beam_tokens': tensor([ 253,  510,  510, 6729]), 'next_beam_indices': tensor([2, 1, 3, 0])}\n",
      "Beam Outputs\n",
      "{'next_beam_scores': tensor([-14.9837, -16.7500, -16.9502, -17.2218]), 'next_beam_tokens': tensor([ 5286, 11772,  2469,  1986]), 'next_beam_indices': tensor([3, 0, 0, 0])}\n",
      "{'input_ids': tensor([[39302,   369,  5686]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "up model inputs, down output\n",
      "{'sequences': tensor([[39302,   369,  5686,   275,   253,  1986,  2077,    15,   187,   187,\n",
      "           510,  6729,  5286],\n",
      "        [39302,   369,  5686,   275,   253,  1986,  2077,    15,   187,   187,\n",
      "           688,   253, 11772],\n",
      "        [39302,   369,  5686,   275,   253,  1986,  2077,    15,   187,   187,\n",
      "           688,   253,  2469],\n",
      "        [39302,   369,  5686,   275,   253,  1986,  2077,    15,   187,   187,\n",
      "           688,   253,  1986]]), 'sequences_scores': tensor([-1.4984, -1.6750, -1.6950, -1.7222]), 'scores': (tensor([[-1229.4202, -1355.1569,    -6.0454,  ..., -1355.1506, -1355.1493,\n",
      "         -1355.1531],\n",
      "        [-1229.4202, -1355.1569,    -6.0454,  ..., -1355.1506, -1355.1493,\n",
      "         -1355.1531],\n",
      "        [-1229.4202, -1355.1569,    -6.0454,  ..., -1355.1506, -1355.1493,\n",
      "         -1355.1531],\n",
      "        [-1229.4202, -1355.1569,    -6.0454,  ..., -1355.1506, -1355.1493,\n",
      "         -1355.1531]]), tensor([[-1230.0876, -1355.3499,   -10.0932,  ..., -1355.3436, -1355.3424,\n",
      "         -1355.3462],\n",
      "        [-1229.6732, -1355.1931,    -9.7573,  ..., -1355.1862, -1355.1858,\n",
      "         -1355.1887],\n",
      "        [-1228.3949, -1353.7924,    -9.6082,  ..., -1353.7866, -1353.7854,\n",
      "         -1353.7888],\n",
      "        [-1230.0845, -1356.6011,    -8.4123,  ..., -1356.5952, -1356.5940,\n",
      "         -1356.5970]]), tensor([[-1226.8854, -1351.4984,   -11.8418,  ..., -1351.4932, -1351.4918,\n",
      "         -1351.4958],\n",
      "        [-1231.2228, -1356.9410,    -7.2332,  ..., -1356.9354, -1356.9338,\n",
      "         -1356.9376],\n",
      "        [-1226.3002, -1351.0316,   -11.5828,  ..., -1351.0262, -1351.0248,\n",
      "         -1351.0284],\n",
      "        [-1221.0641, -1345.4105,   -11.8828,  ..., -1345.4037, -1345.4039,\n",
      "         -1345.4071]]), tensor([[-1230.3573, -1355.9711,   -10.3417,  ..., -1355.9652, -1355.9641,\n",
      "         -1355.9675],\n",
      "        [-1229.9662, -1356.4344,    -5.5362,  ..., -1356.4285, -1356.4271,\n",
      "         -1356.4310],\n",
      "        [-1223.6432, -1348.1151,   -13.1070,  ..., -1348.1086, -1348.1083,\n",
      "         -1348.1119],\n",
      "        [-1215.1702, -1340.6003,   -12.0252,  ..., -1340.5935, -1340.5933,\n",
      "         -1340.5963]]), tensor([[-1230.5972, -1356.9691,    -5.7840,  ..., -1356.9629, -1356.9622,\n",
      "         -1356.9656],\n",
      "        [-1228.3337, -1353.7262,   -10.6159,  ..., -1353.7202, -1353.7186,\n",
      "         -1353.7224],\n",
      "        [-1230.5386, -1356.6388,    -5.6472,  ..., -1356.6328, -1356.6316,\n",
      "         -1356.6354],\n",
      "        [-1227.2404, -1352.0537,   -11.5175,  ..., -1352.0486, -1352.0471,\n",
      "         -1352.0508]]), tensor([[-1230.6421, -1356.5328,   -10.1472,  ..., -1356.5271, -1356.5256,\n",
      "         -1356.5298],\n",
      "        [-1226.1611, -1352.5690,   -10.9438,  ..., -1352.5620, -1352.5610,\n",
      "         -1352.5641],\n",
      "        [-1230.3221, -1355.8433,   -10.2437,  ..., -1355.8372, -1355.8363,\n",
      "         -1355.8394],\n",
      "        [-1230.3524, -1356.1223,   -10.0900,  ..., -1356.1163, -1356.1151,\n",
      "         -1356.1190]]), tensor([[-1232.2083, -1359.2548,   -10.7929,  ..., -1359.2490, -1359.2483,\n",
      "         -1359.2515],\n",
      "        [-1230.9630, -1356.6616,    -9.8207,  ..., -1356.6555, -1356.6544,\n",
      "         -1356.6580],\n",
      "        [-1231.5254, -1357.3899,    -9.6935,  ..., -1357.3840, -1357.3826,\n",
      "         -1357.3859],\n",
      "        [-1231.4020, -1357.1545,   -10.1384,  ..., -1357.1483, -1357.1472,\n",
      "         -1357.1508]]), tensor([[-1227.3135, -1354.2573,   -10.1080,  ..., -1354.2522, -1354.2509,\n",
      "         -1354.2542],\n",
      "        [-1227.9012, -1352.3400,   -12.0000,  ..., -1352.3345, -1352.3331,\n",
      "         -1352.3373],\n",
      "        [-1230.1521, -1355.9479,   -10.1558,  ..., -1355.9415, -1355.9410,\n",
      "         -1355.9445],\n",
      "        [-1228.3192, -1352.9386,   -13.2432,  ..., -1352.9331, -1352.9313,\n",
      "         -1352.9355]]), tensor([[-1229.1075, -1353.6594,   -12.3930,  ..., -1353.6534, -1353.6525,\n",
      "         -1353.6559],\n",
      "        [-1228.5934, -1355.6055,   -11.6500,  ..., -1355.6008, -1355.5990,\n",
      "         -1355.6025],\n",
      "        [-1229.3678, -1354.5393,   -12.8251,  ..., -1354.5327, -1354.5314,\n",
      "         -1354.5356],\n",
      "        [-1229.1259, -1356.2777,   -11.1844,  ..., -1356.2729, -1356.2714,\n",
      "         -1356.2745]]), tensor([[-1227.4689, -1352.3667,   -13.2414,  ..., -1352.3617, -1352.3596,\n",
      "         -1352.3636],\n",
      "        [-1229.2385, -1353.8967,   -12.4324,  ..., -1353.8907, -1353.8900,\n",
      "         -1353.8933],\n",
      "        [-1229.2939, -1353.8147,   -12.7466,  ..., -1353.8085, -1353.8079,\n",
      "         -1353.8110],\n",
      "        [-1224.5291, -1349.7988,   -12.0906,  ..., -1349.7919, -1349.7920,\n",
      "         -1349.7954]])), 'beam_indices': tensor([[ 0,  0,  0,  2,  0,  1,  0,  0,  0,  3, -1, -1, -1],\n",
      "        [ 0,  0,  0,  2,  0,  1,  0,  0,  2,  0, -1, -1, -1],\n",
      "        [ 0,  0,  0,  2,  0,  1,  0,  0,  2,  0, -1, -1, -1],\n",
      "        [ 0,  0,  0,  2,  0,  1,  0,  0,  2,  0, -1, -1, -1]])}\n",
      "torch.Size([4, 50304])\n",
      "**************************************** Next generation\n",
      "########################################Output:\n",
      "Obama was born in the United States.\n",
      "\n",
      "The Obama administration\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"max_new_tokens\": 10,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_return_sequences\": 4,\n",
      "  \"resume_generation\": true,\n",
      "  \"return_dict_in_generate\": true\n",
      "}\n",
      " {'input_ids': tensor([[39302,   369,  5686]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "Resuming generation\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'result' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;241m40\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 41\u001b[0m output2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresume_generation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# print(f\"Time elapsed: {elapsed_time:.2f} seconds\")\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\helle\\micromamba\\envs\\sem-hf\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\programmieren\\ma\\semantic_bs\\transformers\\src\\transformers\\generation\\utils.py:2140\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, (DynamicCache, EncoderDecoderCache)):\n\u001b[0;32m   2139\u001b[0m             result\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mpast_key_values\u001b[38;5;241m.\u001b[39mto_legacy_cache()\n\u001b[1;32m-> 2140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresult\u001b[49m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: cannot access local variable 'result' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "print(\"Starting model downloads\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "for model_name in checkpoints:\n",
    "    print(40 * \"#\")\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, token=access_token, device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "    print(f\"Model {model_name} loaded successfully\")\n",
    "\n",
    "    example = \"Obama was born\"\n",
    "\n",
    "    # print(f\"Generating model inputs\")\n",
    "    model_inputs = tokenizer(example, return_tensors=\"pt\").to(device)\n",
    "    # print(f\"Generating output\")\n",
    "\n",
    "    inference_start_time = time.time()\n",
    "    output = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=10,\n",
    "        num_beams=4,\n",
    "        num_return_sequences=4,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores = True,\n",
    "        # output_attentions = True\n",
    "        )\n",
    "    print(model_inputs)\n",
    "    print(\"up model inputs, down output\")\n",
    "    # print output but do not show \"past_key_values\" and \"decoder_hidden_states\" keys\n",
    "    print({k: v for k, v in output.items() if k not in [\"past_key_values\", \"decoder_hidden_states\"]})\n",
    "    print(output.scores[0].shape)\n",
    "    print(40 * \"*\", \"Next generation\")\n",
    "    inference_elapsed_time = time.time() - inference_start_time\n",
    "\n",
    "    print(40 * \"#\" + f\"Output:\")\n",
    "    print(tokenizer.batch_decode(output[0], skip_special_tokens=True)[0])\n",
    "    output2 = model.generate(**model_inputs, max_new_tokens=10, num_beams=4, num_return_sequences=4, return_dict_in_generate=True, resume_generation = True)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    # print(f\"Time elapsed: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Inference time: {inference_elapsed_time:.2f} seconds\")\n",
    "    # print(f\"Done with {model_name}!\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sem-hf",
   "language": "python",
   "name": "sem-hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
